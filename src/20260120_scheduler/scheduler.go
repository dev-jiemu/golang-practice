package main

import (
	"context"
	"sort"
	"sync"
	"time"

	log "github.com/sirupsen/logrus"
)

type Task struct {
	ID     string
	UserID string
	Status string // "pending", "Processing", "Pending", "completed"
}

type UserStat struct {
	UserID       string
	PendingCount int
	RunningCount int
	LastUpdated  time.Time
}

// UserQuota : ë™ì  ê³µì •ë¶„ë°°
type UserQuota struct {
	UserID       string
	MaxSlots     int
	CurrentUsage int
	LastUpdated  time.Time
}

type Scheduler struct {
	config *Config

	// ì¸ë©”ëª¨ë¦¬ ìŠ¤í† ë¦¬ì§€ (POCìš©)
	tasks   map[string]*Task // taskID -> Task
	tasksMu sync.RWMutex

	// â˜… dispatchëœ task ê´€ë¦¬ (ë‚´ë¶€ í ì‹œë®¬ë ˆì´ì…˜)
	dispatchedTasks map[string]*Task // taskID -> Task
	dispatchedMu    sync.RWMutex

	// í†µê³„ ìºì‹œ
	userStats map[string]*UserStat
	statsMu   sync.RWMutex

	// ë™ì  í• ë‹¹ëŸ‰ ê´€ë¦¬
	userQuotas     map[string]*UserQuota
	quotaForShared int // ê³µìš© ì˜ì—­ í• ë‹¹ëŸ‰
	quotaMu        sync.RWMutex
}

func NewScheduler(config *Config) *Scheduler {
	return &Scheduler{
		config:          config,
		tasks:           make(map[string]*Task),
		dispatchedTasks: make(map[string]*Task),
		userStats:       make(map[string]*UserStat),
		userQuotas:      make(map[string]*UserQuota),
	}
}

func (v *Scheduler) Start(ctx context.Context) {
	ticker := time.NewTicker(1 * time.Second)
	defer ticker.Stop()

	// í†µê³„ ê°±ì‹ ìš© ê³ ë£¨í‹´
	go v.startStatRefresher(ctx)

	for {
		select {
		case <-ctx.Done():
			return
		case <-ticker.C:
			if err := v.processBatch(); err != nil {
				log.Errorf("batch error: %v", err)
			}
		}
	}
}

func (v *Scheduler) processBatch() error {
	// ProcessingCount -> PendingCount ìˆœì„œë¡œ ë¶„ë°°
	v.dispatchedMu.RLock()
	processingCount := 0
	pendingCount := 0
	for _, task := range v.dispatchedTasks {
		if task.Status == "Processing" {
			processingCount++
		} else if task.Status == "Pending" {
			pendingCount++
		}
	}
	v.dispatchedMu.RUnlock()

	// 1. ProcessingCount ë¨¼ì € ë¶„ë°°
	processingAvailable := v.config.ProcessingCount - processingCount
	if processingAvailable > 0 {
		processingTasks := v.allocateTasks(processingAvailable, "Processing")
		_ = v.dispatchTasks(processingTasks, "Processing")
	}

	// 2. PendingCount ë¶„ë°°
	pendingAvailable := v.config.PendingCount - pendingCount
	if pendingAvailable > 0 {
		pendingTasks := v.allocateTasks(pendingAvailable, "Pending")
		_ = v.dispatchTasks(pendingTasks, "Pending")
	}

	return nil
}

func (v *Scheduler) allocateTasks(available int, status string) []*Task {
	v.statsMu.RLock()
	userCount := len(v.userStats)
	v.statsMu.RUnlock()

	if userCount == 0 || available <= 0 {
		return []*Task{}
	}

	// ìœ ì €ë³„ pending task ê°œìˆ˜ ì¡°íšŒ (pendingë§Œ)
	v.statsMu.RLock()
	allUsers := make([]*UserStat, 0, len(v.userStats))
	for _, stat := range v.userStats {
		if stat.PendingCount > 0 {
			allUsers = append(allUsers, stat)
		}
	}
	v.statsMu.RUnlock()

	if len(allUsers) == 0 {
		return []*Task{}
	}

	// PendingCount ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ (ë§ì€ ìˆœì„œ)
	sort.Slice(allUsers, func(i, j int) bool {
		return allUsers[i].PendingCount > allUsers[j].PendingCount
	})

	userCount = len(allUsers)
	maxDedicated := v.config.MaxDedicatedUsers
	tasks := make([]*Task, 0, available)

	// Case 1: ìœ ì € ìˆ˜ <= MaxDedicatedUsers (ëª¨ë‘ ë™ì¼í•˜ê²Œ ë¶„ë°°)
	if userCount <= maxDedicated {
		perUser := available / userCount
		remainder := available % userCount

		for i, user := range allUsers {
			quota := perUser
			if i < remainder {
				quota++
			}
			if quota > 0 {
				userTasks := v.fetchUserPendingTasksFIFO(user.UserID, quota)
				tasks = append(tasks, userTasks...)
			}
		}
		log.Debugf("[%s] %d users (â‰¤ %d), equal distribution: %d per user",
			status, userCount, maxDedicated, perUser)
		return tasks
	}

	// Case 2: ìœ ì € ìˆ˜ > MaxDedicatedUsers
	// Dedicated ì˜ì—­: ìƒìœ„ MaxDedicatedUsersëª…ì—ê²Œ DedicatedQuotaPercentë§Œí¼ í• ë‹¹
	dedicatedQuota := int(float64(available)*v.config.DedicatedQuotaPercent + 0.5) // ë°˜ì˜¬ë¦¼
	if dedicatedQuota < maxDedicated {
		dedicatedQuota = maxDedicated // ìµœì†Œí•œ 1ê°œì”©ì€ ë³´ì¥
	}
	perDedicated := dedicatedQuota / maxDedicated

	dedicatedUsers := allUsers[:maxDedicated]
	sharedUsers := allUsers[maxDedicated:]

	// Dedicated ìœ ì €ë“¤ì—ê²Œ í• ë‹¹ (FIFO ìˆœì„œë¡œ ë°œí–‰)
	dedicatedAllocated := 0
	for _, user := range dedicatedUsers {
		if perDedicated > 0 {
			userTasks := v.fetchUserPendingTasksFIFO(user.UserID, perDedicated)
			tasks = append(tasks, userTasks...)
			dedicatedAllocated += len(userTasks)
		}
	}

	// Shared ì˜ì—­: ë‚¨ì€ ìŠ¬ë¡¯ ê³„ì‚°
	sharedQuota := available - dedicatedAllocated

	// Shared ìœ ì €ë“¤ì„ ìš”ì²­ ì ì€ ìˆœìœ¼ë¡œ ì •ë ¬
	sort.Slice(sharedUsers, func(i, j int) bool {
		return sharedUsers[i].PendingCount < sharedUsers[j].PendingCount
	})

	// Shared ìœ ì €ë“¤ì—ê²Œ ìš”ì²­ ì ì€ ìˆœìœ¼ë¡œ round-robin ë°©ì‹ í• ë‹¹
	if sharedQuota > 0 && len(sharedUsers) > 0 {
		perShared := sharedQuota / len(sharedUsers)
		remainder := sharedQuota % len(sharedUsers)

		for i, user := range sharedUsers {
			quota := perShared
			if i < remainder {
				quota++
			}
			if quota > 0 {
				userTasks := v.fetchUserPendingTasksFIFO(user.UserID, quota)
				tasks = append(tasks, userTasks...)
			}
		}
	}

	log.Debugf("[%s] %d users (> %d MaxDedicated): dedicated=%d users (quota=%d each), shared=%d users (quota=%d total)",
		status, userCount, maxDedicated, maxDedicated, perDedicated, len(sharedUsers), sharedQuota)

	// Dedicated <-> Shared êµì²´ ì²´í¬
	if len(sharedUsers) > 0 && len(dedicatedUsers) > 0 {
		// Sharedì—ì„œ ê°€ì¥ ë§ì€ ìœ ì € (ì •ë ¬ í›„ ë§ˆì§€ë§‰)
		largestShared := sharedUsers[len(sharedUsers)-1]
		// Dedicatedì—ì„œ ê°€ì¥ ì ì€ ìœ ì € (ì •ë ¬ ì‹œ ë§ˆì§€ë§‰)
		smallestDedicated := dedicatedUsers[len(dedicatedUsers)-1]

		if largestShared.PendingCount > smallestDedicated.PendingCount {
			log.Infof("[%s] ğŸ”„ Swap candidate: shared[%s]=%d > dedicated[%s]=%d (will swap in next cycle)",
				status, largestShared.UserID, largestShared.PendingCount,
				smallestDedicated.UserID, smallestDedicated.PendingCount)
			// ì‹¤ì œ êµì²´ëŠ” ë‹¤ìŒ í†µê³„ ê°±ì‹  ì‹œ ìë™ìœ¼ë¡œ ë°˜ì˜ë¨ (ì •ë ¬ ê¸°ì¤€ì´ PendingCountì´ë¯€ë¡œ)
		}
	}

	return tasks
}

// fetchUserPendingTasksFIFO : íŠ¹ì • ìœ ì €ì˜ pending taskë¥¼ FIFO ìˆœì„œë¡œ ê°€ì ¸ì˜´
func (v *Scheduler) fetchUserPendingTasksFIFO(userID string, limit int) []*Task {
	v.tasksMu.RLock()
	defer v.tasksMu.RUnlock()

	tasks := make([]*Task, 0, limit)
	// ë‹¨ìˆœíˆ ë„£ì€ ìˆœì„œëŒ€ë¡œ ë°œí–‰ (map iteration orderëŠ” ëœë¤ì´ì§€ë§Œ, í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì¶©ë¶„)
	for _, task := range v.tasks {
		if task.UserID == userID && task.Status == "pending" {
			tasks = append(tasks, task)
			if len(tasks) >= limit {
				break
			}
		}
	}
	return tasks
}

// startStatRefresher : í†µê³„ ê°±ì‹ ìš©
func (v *Scheduler) startStatRefresher(ctx context.Context) {
	ticker := time.NewTicker(v.config.StatRefreshInterval)
	defer ticker.Stop()

	for {
		select {
		case <-ctx.Done():
			return
		case <-ticker.C:
			v.refreshStats()
			v.recalculateQuotas() // í• ë‹¹ëŸ‰ ì¬ê³„ì‚°
		}
	}
}

func (v *Scheduler) recalculateQuotas() {
	// ì´ì œëŠ” í•„ìš” ì—†ì§€ë§Œ ë‚˜ì¤‘ì„ ìœ„í•´ ìœ ì§€
	// allocateTasksì—ì„œ ì§ì ‘ ê³„ì‚°í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” í†µê³„ë§Œ ê°±ì‹ 
	log.Debugf("Quota recalculation triggered (stats refreshed)")
}

// refreshStats : í†µê³„ ì¡°íšŒ (POC : in-memory)
func (v *Scheduler) refreshStats() {
	v.tasksMu.RLock()
	defer v.tasksMu.RUnlock()

	userCounts := make(map[string]*UserStat)

	for _, task := range v.tasks {
		if userCounts[task.UserID] == nil {
			userCounts[task.UserID] = &UserStat{
				UserID:      task.UserID,
				LastUpdated: time.Now(),
			}
		}

		switch task.Status {
		case "pending":
			userCounts[task.UserID].PendingCount++
		case "Processing", "Pending":
			userCounts[task.UserID].RunningCount++
		}
	}

	v.statsMu.Lock()
	v.userStats = userCounts
	v.statsMu.Unlock()
}

func (v *Scheduler) dispatchTasks(tasks []*Task, status string) error {
	if len(tasks) == 0 {
		return nil
	}

	log.Printf("[%s] dispatching %d tasks", status, len(tasks))

	// dispatchedTasksì— ì¶”ê°€
	v.dispatchedMu.Lock()
	for _, task := range tasks {
		task.Status = status
		v.dispatchedTasks[task.ID] = task
		log.Printf("  - task=%s user=%s status=%s", task.ID, task.UserID, status)
	}
	v.dispatchedMu.Unlock()

	// ì›ë³¸ tasksì—ì„œë„ ìƒíƒœ ì—…ë°ì´íŠ¸
	v.tasksMu.Lock()
	for _, task := range tasks {
		if originalTask, exists := v.tasks[task.ID]; exists {
			originalTask.Status = status
		}
	}
	v.tasksMu.Unlock()

	// ê° taskë§ˆë‹¤ ìƒíƒœ ëª¨ë‹ˆí„°ë§ ê³ ë£¨í‹´ ì‹œì‘
	for _, task := range tasks {
		go v.monitorTaskStatus(task)
	}

	return nil
}

// monitorTaskStatus : ê°œë³„ taskì˜ ìƒíƒœë¥¼ ëª¨ë‹ˆí„°ë§í•˜ê³  ì™„ë£Œ ì²˜ë¦¬
func (v *Scheduler) monitorTaskStatus(task *Task) {
	if task.Status == "Pending" {
		// 1. Pending â†’ Processing: Queue ëŒ€ê¸° ì‹œê°„
		waitTime := time.Duration(1+time.Now().UnixNano()%3) * time.Second
		time.Sleep(waitTime)

		// 2. Processingìœ¼ë¡œ ìƒíƒœ ë³€ê²½ (Consumerê°€ ì²˜ë¦¬ ì‹œì‘)
		v.updateTaskStatusOnly(task.ID, "Processing")
		log.Debugf("â†’ Status changed: task=%s Pending â†’ Processing (waited %v)",
			task.ID, waitTime)

		// 3. Processing ì²˜ë¦¬ ì‹œê°„
		processingTime := time.Duration(1+time.Now().UnixNano()%10) * time.Second
		time.Sleep(processingTime)

		// 4. Completedë¡œ ë³€ê²½ ë° Queueì—ì„œ ì œê±°
		v.updateTaskStatus(task.ID, "completed")
		log.Debugf("âœ“ Task completed: task=%s user=%s (wait=%v, process=%v)",
			task.ID, task.UserID, waitTime, processingTime)
	} else {
		// Processingì€ ë°”ë¡œ ì²˜ë¦¬ ì‹œì‘
		processingTime := time.Duration(1+time.Now().UnixNano()%10) * time.Second
		time.Sleep(processingTime)

		// Completedë¡œ ë³€ê²½ ë° Queueì—ì„œ ì œê±°
		v.updateTaskStatus(task.ID, "completed")
		log.Debugf("âœ“ Task completed: task=%s user=%s (took %v)",
			task.ID, task.UserID, processingTime)
	}
}

// updateTaskStatusOnly : ìƒíƒœë§Œ ì—…ë°ì´íŠ¸ (dispatchedTasksì—ì„œ ì œê±°í•˜ì§€ ì•ŠìŒ)
func (v *Scheduler) updateTaskStatusOnly(taskID string, newStatus string) {
	// ì›ë³¸ tasks ìƒíƒœ ì—…ë°ì´íŠ¸ (DB ì—…ë°ì´íŠ¸ ì‹œë®¬ë ˆì´ì…˜)
	v.tasksMu.Lock()
	if task, exists := v.tasks[taskID]; exists {
		task.Status = newStatus
	}
	v.tasksMu.Unlock()

	// dispatchedTasksë„ ìƒíƒœ ì—…ë°ì´íŠ¸ (ì œê±°ëŠ” ì•ˆ í•¨)
	v.dispatchedMu.Lock()
	if task, exists := v.dispatchedTasks[taskID]; exists {
		task.Status = newStatus
	}
	v.dispatchedMu.Unlock()
}

// updateTaskStatus : task ìƒíƒœ ì—…ë°ì´íŠ¸ ë° dispatchedTasksì—ì„œ ì œê±°
func (v *Scheduler) updateTaskStatus(taskID string, newStatus string) {
	// 1. dispatchedTasksì—ì„œ ì œê±° (Queueì—ì„œ ì™„ë£Œë¨)
	v.dispatchedMu.Lock()
	delete(v.dispatchedTasks, taskID)
	remaining := len(v.dispatchedTasks)
	v.dispatchedMu.Unlock()

	// 2. ì›ë³¸ tasks ìƒíƒœ ì—…ë°ì´íŠ¸ (DB ì—…ë°ì´íŠ¸ ì‹œë®¬ë ˆì´ì…˜)
	v.tasksMu.Lock()
	if task, exists := v.tasks[taskID]; exists {
		task.Status = newStatus
	}
	v.tasksMu.Unlock()

	log.Debugf("âœ“ Status updated: task=%s â†’ %s (queue remaining: %d)",
		taskID, newStatus, remaining)
}

// ========== í…ŒìŠ¤íŠ¸ìš© í—¬í¼ ==========

func (v *Scheduler) AddTask(task *Task) {
	v.tasksMu.Lock()
	defer v.tasksMu.Unlock()
	v.tasks[task.ID] = task
}
